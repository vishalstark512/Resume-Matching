{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efed46e0-1494-4df4-b805-92d8af501726",
   "metadata": {},
   "source": [
    "# AI Pipeline for Auto Resume Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba676e-d52a-4f81-a8bc-f570e4ab7573",
   "metadata": {},
   "source": [
    "# Strategy I\n",
    "\n",
    "### Pre-processing steps:\n",
    "\n",
    "1. **JSON to DataFrame**: The job description JSON file is converted into a pandas data frame to simplify data manipulation.\n",
    "2. **Text Cleaning**: The code employs various natural language processing operations such as tokenization, stemming using WordNetLemmatizer, and removal of punctuations. A peculiar pattern of recurring \"AAAA\" is specifically cleaned from the text. There's a specific focus on retaining certain words, namely 'SAP', 'S4Hana', and 'ICT', and these are excluded from the stopwords list.\n",
    "3. **Language Translation**: To maintain consistency, any resume in a non-English language is converted to English.\n",
    "4. **Noise Reduction**: To ensure the text is meaningful and relevant, only words that are recognized in the English dictionary are retained, effectively filtering out any noise or non-sensical terms.\n",
    "\n",
    "### Overall Strategy:\n",
    "\n",
    "1. **Feature Extraction via Regex Patterns**: Before embedding the text, extract domain-specific features using predefined regex patterns.\n",
    "2. **Embedding Texts**: Utilize a pre-trained SentenceTransformer model (`paraphrase-MiniLM-L6-v2`) to embed the textual content.\n",
    "3. **Chunking Long Texts**: For texts that exceed the model's token limit, divide the text into overlapping chunks, embed each chunk separately, and then average their embeddings.\n",
    "4. **Combining Embeddings**: Combine the embeddings obtained from the SentenceTransformer model with the domain-specific feature embeddings.\n",
    "5. **Similarity Calculation**: Compute the cosine similarity between the job requirement's combined embedding and the combined embedding of each resume in the dataset.\n",
    "6. **Ranking**: Rank the resumes based on their similarity scores to the job requirement.\n",
    "7. **Output**: Save the detailed similarity scores and rankings to a CSV file and prepare a final submission CSV with the ID and rank.\n",
    "\n",
    "### Cool Optimizations and Challenges Solved:\n",
    "\n",
    "1. **Overlapping Chunking**:\n",
    "    - *Challenge*: Direct chunking could miss some contextual information at the borders.\n",
    "    - *Solution*: The implementation uses overlapping chunks to ensure that no information is lost at the boundaries between chunks. The average of the embeddings of these overlapping chunks provides a more holistic representation of the content.\n",
    "\n",
    "2. **Domain-Specific Feature Embeddings**:\n",
    "    - *Challenge*: Simple embeddings might not capture domain-specific nuances.\n",
    "    - *Solution*: By defining a set of domain-specific features and using regex patterns to extract these features, the strategy combines traditional feature extraction with modern embedding techniques. This ensures that both general and domain-specific information is captured.\n",
    "\n",
    "3. **Optimized Tokenization**:\n",
    "    - *Challenge*: Direct string chunking may not consider word or sentence boundaries.\n",
    "    - *Solution*: By tokenizing the text first and then creating chunks based on token count, the implementation ensures that words or sentences aren't arbitrarily split, preserving their meaning.\n",
    "\n",
    "4. **Weighted Job Requirements**:\n",
    "    - *Challenge*: Some job requirements might be more important than others.\n",
    "    - *Solution*: The code multiplies the \"must_have\" and \"should_have\" requirements by 2, effectively giving them more weight in the embedding process. This highlights the importance of these requirements in the similarity calculations.\n",
    "\n",
    "5. **Efficient Similarity Calculation**:\n",
    "    - *Challenge*: Computing cosine similarity in a naive way can be computationally expensive.\n",
    "    - *Solution*: The use of `util.pytorch_cos_sim` provides an efficient way to compute cosine similarities using PyTorch, thereby speeding up the overall process.\n",
    "\n",
    "6. **Scalable Ranking System**:\n",
    "    - *Challenge*: With many resumes, establishing a ranking system can become challenging.\n",
    "    - *Solution*: The pandas `.rank()` function is employed to efficiently rank resumes based on their similarity scores, ensuring the solution remains scalable as the dataset size grows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d303e007-0ce2-4baa-aebb-a170d4885e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Union, Dict\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from translate import Translator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b560b124-4b64-42b7-bfbb-2fd3d1f0e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_dataframe(json_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Convert a JSON file into a pandas DataFrame.\"\"\"\n",
    "    # Open and load the JSON file\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for item in data:\n",
    "        # Extracting data from the JSON item and creating a record dictionary\n",
    "        record = {\n",
    "            \"_id\": str(item[\"_id\"][\"$oid\"]),  # Assuming _id is a string\n",
    "            \"location\": item[\"location\"],\n",
    "            \"title\": item[\"title\"],\n",
    "            \"organisation\": item[\"organisation\"],\n",
    "            \"description\": item[\"description\"]\n",
    "        }\n",
    "\n",
    "        # Extracting specific_skills from the JSON item and categorizing them\n",
    "        specific_skills = item[\"specific_skills\"]\n",
    "        nice_to_have = [skill[\"title\"] for skill in specific_skills if skill[\"weigth\"] == \"Nice to have\"]\n",
    "        should_have = [skill[\"title\"] for skill in specific_skills if skill[\"weigth\"] == \"Should have\"]\n",
    "        record[\"nice_to_have\"] = nice_to_have\n",
    "        record[\"should_have\"] = should_have\n",
    "\n",
    "        # Extracting sector_skills from the JSON item and categorizing them\n",
    "        sector_skills = item[\"sector_skills\"]\n",
    "        must_have = [skill[\"title\"] for skill in sector_skills if skill[\"weigth\"] == \"Must have\"]\n",
    "        record[\"must_have\"] = must_have\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    # Creating a pandas DataFrame from the list of records\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_string(text: str) -> str:\n",
    "    \"\"\"Process a given string by translating to English, removing punctuations,\n",
    "    lemmatizing each word, and removing stopwords (excluding 'SAP', 'S4Hana', and 'ICT').\"\"\"\n",
    "    # Translating the text to English\n",
    "    translator = Translator(to_lang='en')\n",
    "    text = translator.translate(text)\n",
    "\n",
    "    # Tokenizing the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Create a set of special words you want to keep\n",
    "    special_words = {'SAP', 'S4Hana', 'ICT'}\n",
    "\n",
    "    # Converting tokens to lowercase only if they are not in the set of special words\n",
    "    tokens = [token.lower() if token not in special_words else token for token in tokens]\n",
    "\n",
    "    # Replace 'sa' with 'SAP'\n",
    "    tokens = ['SAP' if token.lower() == 'sa' else token for token in tokens]\n",
    "\n",
    "    # Removing punctuations from the text\n",
    "    tokens = [''.join([c for c in token if c not in string.punctuation]) for token in tokens]\n",
    "\n",
    "    # Removing stopwords and lemmatizing each word\n",
    "    stop_words = set(stopwords.words('english')) - {word.lower() for word in special_words}\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Removing words that are not in the English dictionary\n",
    "    english_words = set(words.words())\n",
    "    processed_tokens = [token for token in processed_tokens if token in english_words or token in special_words]\n",
    "\n",
    "    # Joining the processed tokens back into a single string\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def preprocess_text(data) -> list or str:\n",
    "    \"\"\"Preprocess a single string or a list of strings by applying process_string function.\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        # If input is a list, process each string in the list using process_string function\n",
    "        return [process_string(s) for s in data]\n",
    "    elif isinstance(data, str):\n",
    "        # If input is a single string, process it using process_string function\n",
    "        return process_string(data)\n",
    "    else:\n",
    "        # Raise an error if the input is not a string or a list\n",
    "        raise ValueError(\"Input must be a string or list of strings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280d4b95-4b6f-46e4-9b11-fc75309eabda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumes and job description processed...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>resume_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>socrates</td>\n",
       "      <td>contact mobile top skill business development ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pythagoras</td>\n",
       "      <td>contact mobile company top skill service manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heraclitus</td>\n",
       "      <td>business analysis functional analysis requirem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>homer</td>\n",
       "      <td>contact top skill business analysis language d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parmenides</td>\n",
       "      <td>business transformation program management inn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                        resume_text\n",
       "0    socrates  contact mobile top skill business development ...\n",
       "1  pythagoras  contact mobile company top skill service manag...\n",
       "2  heraclitus  business analysis functional analysis requirem...\n",
       "3       homer  contact top skill business analysis language d...\n",
       "4  parmenides  business transformation program management inn..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Converting Job requirements JSON into a dataframe\n",
    "json_file = 'job_description_response.json'\n",
    "df_req = convert_json_to_dataframe(json_file)\n",
    "df_req.drop(['_id', 'location', \"organisation\"], axis=1, inplace=True)\n",
    "df_req['description'] = df_req['description'].apply(preprocess_text)\n",
    "df_req['nice_to_have'] = df_req['nice_to_have'].apply(preprocess_text)\n",
    "df_req['should_have'] = df_req['should_have'].apply(preprocess_text)\n",
    "df_req['must_have'] = df_req['must_have'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Load resume data\n",
    "df_resume = pd.read_csv(\"resumes.csv\", delimiter=',', quotechar='\"')\n",
    "\n",
    "# Preprocess the resume text\n",
    "df_resume[' resume_text'] = df_resume[' resume_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Resumes and job description processed...\")\n",
    "\n",
    "df_resume.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc9b82c5-d7e8-44ea-9d80-2452376600f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the list of features and corresponding regex patterns or direct strings\n",
    "features = {\n",
    "    \"s4hana_experience\": r'\\bS4Hana\\b',\n",
    "    \"sap_experience\": r'\\bSAP\\b',\n",
    "    \"energy_sector_experience\": r'\\b(?:Belgian )?energy sector\\b',\n",
    "    \"business_analysis_experience\": r'\\bbusiness analyst\\b',\n",
    "    \"bpm_experience\": r'\\bBPM\\b|business process(?:es)?(?: development| optimization)?\\b',\n",
    "    \"stakeholder_management\": r'\\bstakeholder management\\b',\n",
    "    \"process_analysis\": r'\\banalyz(?:e|ed|es|ing)\\b processes\\b',\n",
    "    \"documentation_skills\": r'\\buse cases?\\b|user manuals?\\b',\n",
    "    \"testing_skills\": r'\\bsupervis(?:e|ing)? testing\\b|testing yourself\\b',\n",
    "    \"training_skills\": r'\\bprovid(?:e|ing)? training\\b|supporting trainers\\b|training key users\\b',\n",
    "    \"writing_skills\": r'\\bwriting processes\\b',\n",
    "    \"training_end_users\": r'\\btraining end users\\b'\n",
    "}\n",
    "\n",
    "\n",
    "def extract_features_from_text(text: str, features: dict) -> dict:\n",
    "    \"\"\"Extract the presence of features from a text based on regex patterns or direct strings.\"\"\"\n",
    "    extracted_features = {}\n",
    "    for feature, pattern in features.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            extracted_features[feature] = 1  # Feature present\n",
    "        else:\n",
    "            extracted_features[feature] = 0  # Feature not present\n",
    "    return extracted_features\n",
    "\n",
    "\n",
    "def feature_dict_to_embedding(feature_dict: dict) -> List[int]:\n",
    "    \"\"\"Convert a dictionary of features to a simple embedding (list of 1s and 0s).\"\"\"\n",
    "    return list(feature_dict.values())\n",
    "\n",
    "def text_to_embedding(model: SentenceTransformer, text: str, features: dict, max_length=512) -> torch.Tensor:\n",
    "    \"\"\" Convert text to its combined embedding using SBERT and additional features with chunking.\"\"\"\n",
    "    # Tokenize the text and check if it exceeds the max_length\n",
    "    tokens = model.tokenize(text)\n",
    "    \n",
    "    if len(tokens) > max_length:\n",
    "        # Split the text into overlapping chunks\n",
    "        stride = int(max_length / 2)\n",
    "        chunks = [text[i:i+max_length] for i in range(0, len(tokens), stride)]\n",
    "        \n",
    "        embeddings = []\n",
    "        for chunk in chunks:\n",
    "            embeddings.append(model.encode(chunk, convert_to_tensor=True))\n",
    "        \n",
    "        # Average the embeddings for all chunks\n",
    "        sbert_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    else:\n",
    "        sbert_embedding = model.encode(text, convert_to_tensor=True)\n",
    "    \n",
    "    extracted_features = extract_features_from_text(text, features)\n",
    "    domain_embedding = torch.Tensor(feature_dict_to_embedding(extracted_features)).to(sbert_embedding.device)\n",
    "    combined_embedding = torch.cat([sbert_embedding, domain_embedding])\n",
    "    return combined_embedding\n",
    "\n",
    "def calculate_similarity(model: SentenceTransformer, req_embedding: torch.Tensor, text: str, features: dict) -> float:\n",
    "    \"\"\" Calculate the cosine similarity between embeddings.\"\"\"\n",
    "    text_embedding = text_to_embedding(model, text, features)\n",
    "    cosine_sim = util.pytorch_cos_sim(req_embedding, text_embedding)\n",
    "    return cosine_sim.item()\n",
    "\n",
    "# Initialize the SBERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Dataframe df_req contains the job requirements data\n",
    "job_requirements = (\n",
    "    df_req[\"title\"] + \" \" + \n",
    "    df_req[\"description\"] + \" \" + \n",
    "    ' '.join(df_req[\"must_have\"][0]*2) + \" \" + \n",
    "    ' '.join(df_req[\"should_have\"][0]*2) + \" \" + \n",
    "    ' '.join(df_req[\"nice_to_have\"][0])\n",
    ")\n",
    "\n",
    "# Generate embeddings for the first job requirement\n",
    "req_embedding = text_to_embedding(model, job_requirements[0], features)\n",
    "\n",
    "\n",
    "# Calculate the similarities between the job requirement and all resumes\n",
    "similarities = [\n",
    "    calculate_similarity(model, req_embedding, resume_row[' resume_text'], features) \n",
    "    for _, resume_row in df_resume.iterrows()\n",
    "]\n",
    "\n",
    "# Update the resume dataframe with similarity scores\n",
    "df_resume['similarity'] = similarities\n",
    "\n",
    "# Rank the resumes based on their similarity scores\n",
    "df_resume['rank'] = df_resume['similarity'].rank(method='min', ascending=False)\n",
    "\n",
    "# Save detailed results to a CSV file\n",
    "df_resume.to_csv('submission_detailed.csv', index=False)\n",
    "\n",
    "# Prepare a subset dataframe for final submission and save it to a CSV file\n",
    "df_subset = df_resume[['id', 'rank']].copy()\n",
    "df_subset['rank'] = df_subset['rank'].astype(int)\n",
    "df_subset.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4705e80f-77fa-4b9c-bb81-ec6a7b9dfa54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>resume_text</th>\n",
       "      <th>similarity</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>socrates</td>\n",
       "      <td>contact mobile top skill business development ...</td>\n",
       "      <td>0.628296</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pythagoras</td>\n",
       "      <td>contact mobile company top skill service manag...</td>\n",
       "      <td>0.742470</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>heraclitus</td>\n",
       "      <td>business analysis functional analysis requirem...</td>\n",
       "      <td>0.493334</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>homer</td>\n",
       "      <td>contact top skill business analysis language d...</td>\n",
       "      <td>0.676995</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parmenides</td>\n",
       "      <td>business transformation program management inn...</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hesiod</td>\n",
       "      <td>contact company top skill functional analysis ...</td>\n",
       "      <td>0.425243</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>theodorus</td>\n",
       "      <td>contact top skill business analysis change man...</td>\n",
       "      <td>0.715598</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>zeno</td>\n",
       "      <td>profile summary manager analyst proven track r...</td>\n",
       "      <td>0.707094</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cyrene</td>\n",
       "      <td>curriculum contact data address postal city te...</td>\n",
       "      <td>0.583185</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elea</td>\n",
       "      <td>contact home top skill related business genera...</td>\n",
       "      <td>0.660715</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>protagoras</td>\n",
       "      <td>contact top skill project management business ...</td>\n",
       "      <td>0.751968</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thomas_baldwin</td>\n",
       "      <td>contact home top skill service management lang...</td>\n",
       "      <td>0.605739</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>alexander_bain</td>\n",
       "      <td>contact personal company top skill visual basi...</td>\n",
       "      <td>0.544304</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>john_bambrough</td>\n",
       "      <td>contact home top skill change management manag...</td>\n",
       "      <td>0.691671</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>david_bell</td>\n",
       "      <td>contact top skill people management project po...</td>\n",
       "      <td>0.635278</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>piers_benn</td>\n",
       "      <td>contact company top skill program management m...</td>\n",
       "      <td>0.561299</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>george_berkeley</td>\n",
       "      <td>mobile agile team work present self employed p...</td>\n",
       "      <td>0.353878</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>isaiah_berlin</td>\n",
       "      <td>contact mobile top skill change management man...</td>\n",
       "      <td>0.603540</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gorgias</td>\n",
       "      <td>contact top skill project management language ...</td>\n",
       "      <td>0.573316</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>francis_bacon</td>\n",
       "      <td>contact top skill project management agile met...</td>\n",
       "      <td>0.693991</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>simon_blackburn</td>\n",
       "      <td>contact top skill office certification relatio...</td>\n",
       "      <td>0.460472</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>george_boole</td>\n",
       "      <td>personal data date birth aa address trilingual...</td>\n",
       "      <td>0.629251</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mary_boole</td>\n",
       "      <td>born driving car business functional analyst t...</td>\n",
       "      <td>0.612128</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fh_bradley</td>\n",
       "      <td>senior strategy advisor consultant project man...</td>\n",
       "      <td>0.694946</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rb_braithwaite</td>\n",
       "      <td>contact top skill analytical skill project man...</td>\n",
       "      <td>0.585704</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ray_brassier</td>\n",
       "      <td>year experience nationality language dutch con...</td>\n",
       "      <td>0.679774</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cd_broad</td>\n",
       "      <td>contact mobile top skill portfolio project pro...</td>\n",
       "      <td>0.651213</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>john_broome</td>\n",
       "      <td>contact top skill change management project ma...</td>\n",
       "      <td>0.706105</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>thomas_browne</td>\n",
       "      <td>senior project program portfolio manager analy...</td>\n",
       "      <td>0.727754</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>roger_bacon</td>\n",
       "      <td>project management development language elemen...</td>\n",
       "      <td>0.666477</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>julian_baggini</td>\n",
       "      <td>contact top skill project management business ...</td>\n",
       "      <td>0.768684</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>john_wycliffe</td>\n",
       "      <td>business analysis functional analysis energy m...</td>\n",
       "      <td>0.673321</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                        resume_text  \\\n",
       "0          socrates  contact mobile top skill business development ...   \n",
       "1        pythagoras  contact mobile company top skill service manag...   \n",
       "2        heraclitus  business analysis functional analysis requirem...   \n",
       "3             homer  contact top skill business analysis language d...   \n",
       "4        parmenides  business transformation program management inn...   \n",
       "5            hesiod  contact company top skill functional analysis ...   \n",
       "6         theodorus  contact top skill business analysis change man...   \n",
       "7              zeno  profile summary manager analyst proven track r...   \n",
       "8            cyrene  curriculum contact data address postal city te...   \n",
       "9              elea  contact home top skill related business genera...   \n",
       "10       protagoras  contact top skill project management business ...   \n",
       "11   thomas_baldwin  contact home top skill service management lang...   \n",
       "12   alexander_bain  contact personal company top skill visual basi...   \n",
       "13   john_bambrough  contact home top skill change management manag...   \n",
       "14       david_bell  contact top skill people management project po...   \n",
       "15       piers_benn  contact company top skill program management m...   \n",
       "16  george_berkeley  mobile agile team work present self employed p...   \n",
       "17    isaiah_berlin  contact mobile top skill change management man...   \n",
       "18          gorgias  contact top skill project management language ...   \n",
       "19    francis_bacon  contact top skill project management agile met...   \n",
       "20  simon_blackburn  contact top skill office certification relatio...   \n",
       "21     george_boole  personal data date birth aa address trilingual...   \n",
       "22       mary_boole  born driving car business functional analyst t...   \n",
       "23       fh_bradley  senior strategy advisor consultant project man...   \n",
       "24   rb_braithwaite  contact top skill analytical skill project man...   \n",
       "25     ray_brassier  year experience nationality language dutch con...   \n",
       "26         cd_broad  contact mobile top skill portfolio project pro...   \n",
       "27      john_broome  contact top skill change management project ma...   \n",
       "28    thomas_browne  senior project program portfolio manager analy...   \n",
       "29      roger_bacon  project management development language elemen...   \n",
       "30   julian_baggini  contact top skill project management business ...   \n",
       "31    john_wycliffe  business analysis functional analysis energy m...   \n",
       "\n",
       "    similarity  rank  \n",
       "0     0.628296  20.0  \n",
       "1     0.742470   3.0  \n",
       "2     0.493334  29.0  \n",
       "3     0.676995  13.0  \n",
       "4     0.689107  11.0  \n",
       "5     0.425243  31.0  \n",
       "6     0.715598   5.0  \n",
       "7     0.707094   6.0  \n",
       "8     0.583185  25.0  \n",
       "9     0.660715  16.0  \n",
       "10    0.751968   2.0  \n",
       "11    0.605739  22.0  \n",
       "12    0.544304  28.0  \n",
       "13    0.691671  10.0  \n",
       "14    0.635278  18.0  \n",
       "15    0.561299  27.0  \n",
       "16    0.353878  32.0  \n",
       "17    0.603540  23.0  \n",
       "18    0.573316  26.0  \n",
       "19    0.693991   9.0  \n",
       "20    0.460472  30.0  \n",
       "21    0.629251  19.0  \n",
       "22    0.612128  21.0  \n",
       "23    0.694946   8.0  \n",
       "24    0.585704  24.0  \n",
       "25    0.679774  12.0  \n",
       "26    0.651213  17.0  \n",
       "27    0.706105   7.0  \n",
       "28    0.727754   4.0  \n",
       "29    0.666477  15.0  \n",
       "30    0.768684   1.0  \n",
       "31    0.673321  14.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45961a91-20b6-4f74-9deb-10128a92aed2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The provided code effectively integrates advanced sentence embeddings with domain-specific feature extraction to rank resumes against job requirements. By adeptly handling longer texts through chunking and emphasizing critical job criteria, the strategy offers a nuanced yet scalable approach to resume matching. This blend of modern machine learning with traditional text analysis promises a more streamlined and precise recruitment process.\n",
    "\n",
    "#### Further possible improvements:\n",
    "\n",
    "1. **Hyperparameter Tuning**: There's potential for performance improvement by fine-tuning parameters, such as the chunk_size. Adjusting this parameter might produce varying results depending on the data.\n",
    "2. **Weighted Cosine Similarity**: Incorporating a weighted cosine similarity measure might lead to better accuracy, although this would require a benchmark or ground truth to verify the improvements.\n",
    "3. **Use NER and speech tagging, sentiment analysis**: Identify more important parts and later on integrate that in the scoring system.\n",
    "4. **Weighted custom features**: We can play around by giving different importance to custom features we extracted.\n",
    "\n",
    "**NOTE:** We can't do these improvements without getting the validation data or the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea02ffb-692e-475e-a0d1-d084ccd554d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3eb5ece6-2a72-49f8-9d8b-22f695ce78e7",
   "metadata": {},
   "source": [
    "# Strategy II\n",
    "\n",
    "This strategy is quite complex and needs a lot of computing (can train this with 16GB VRAM, I ran this in collab); I did a lot of optimization, like using half-precision and batching of embedding, etc.\n",
    "\n",
    "We are not running this but this might give much better performance, especially on larger datasets. \n",
    "\n",
    "#### In this strategy, we use both BERT and Sentence transformers, but why?\n",
    "\n",
    "**BERT:**\n",
    "BERT is used in this code to extract embeddings from text. The choice of using DistilBERT suggests a preference for a faster and lighter version of BERT. DistilBERT retains much of BERT's performance while being faster and requiring less memory.\n",
    "Given that BERT is great at understanding the context and semantics of sentences, it's a good choice for segmenting resumes and job descriptions to capture the essence.\n",
    "\n",
    "**Sentence Transformers:**\n",
    "While BERT can be used to generate sentence embeddings, it does so by taking the average of the token embeddings, which might not always be the optimal way to represent sentence semantics.\n",
    "Sentence Transformers (like the one used, paraphrase-MiniLM-L6-v2) are specifically trained for generating sentence embeddings and hence usually provide more accurate sentence-level semantic representations.\n",
    "The detailed_comparison function leverages Sentence-BERT to compute the similarity between resume sections and the job description.\n",
    "\n",
    "**NOTE**: Do not run this without 16GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49d969-a4c2-4bd2-8ab4-d06e4b14d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device).half()  # Convert to half precision\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2').to(device).half()  # Convert to half precision\n",
    "\n",
    "\n",
    "def split_into_chunks(text: Union[pd.Series, str, object], max_length: int) -> List[str]:\n",
    "    if isinstance(text, pd.Series):\n",
    "        text = text.to_string()\n",
    "    elif not isinstance(text, str):\n",
    "        print(f\"Unexpected type: {type(text)}\")\n",
    "        text = str(text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        if length + sentence_length > max_length:\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = [sentence]\n",
    "            length = sentence_length\n",
    "        else:\n",
    "            chunk.append(sentence)\n",
    "            length += sentence_length\n",
    "    chunks.append(' '.join(chunk))\n",
    "    return chunks\n",
    "\n",
    "def bert_text_to_embedding(tokenizer, model, text, max_length=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    chunk_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in chunks:\n",
    "            if not batch:\n",
    "                continue\n",
    "            batch_inputs = tokenizer(batch, return_tensors=\"pt\", padding='max_length', max_length=max_length, truncation=True)\n",
    "            \n",
    "            # Just move input_ids to the appropriate device without converting to half precision\n",
    "            batch_inputs['input_ids'] = batch_inputs['input_ids'].to(device)\n",
    "            # Convert attention masks to half precision (optional, but for consistency)\n",
    "            batch_inputs['attention_mask'] = batch_inputs['attention_mask'].to(device).half()\n",
    "            \n",
    "            batch_embedding = model(**batch_inputs).last_hidden_state.mean(dim=1)\n",
    "            chunk_embeddings.append(batch_embedding.cpu())\n",
    "\n",
    "    if not chunk_embeddings:\n",
    "        return torch.zeros((model.config.hidden_size,), device=device).half()  # Convert to half precision\n",
    "    \n",
    "    chunk_embeddings = torch.cat(chunk_embeddings)\n",
    "    document_embedding = torch.mean(chunk_embeddings, dim=0)\n",
    "    return document_embedding.to(device).half()  # Convert to half precision\n",
    "\n",
    "\n",
    "\n",
    "def extract_relevant_sections(tokenizer, model, text, job_description_embedding):\n",
    "    sections = split_into_chunks(text, 32)\n",
    "    section_scores = []\n",
    "\n",
    "    for section in sections:\n",
    "        section_embedding = bert_text_to_embedding(tokenizer, model, section)\n",
    "        similarity = torch.cosine_similarity(section_embedding, job_description_embedding, dim=0).item()\n",
    "        section_scores.append((section, similarity))\n",
    "\n",
    "    sorted_sections = sorted(section_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_sections = [section[0] for section in sorted_sections[:5]]\n",
    "    return \" \".join(top_sections)\n",
    "\n",
    "def detailed_comparison(model, text1, text2):\n",
    "    embedding1 = sbert_text_to_embedding(model, text1)\n",
    "    embedding2 = sbert_text_to_embedding(model, text2)\n",
    "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
    "    return similarity\n",
    "\n",
    "def sbert_text_to_embedding(model, text):\n",
    "    embedding = model.encode(text, convert_to_tensor=True)\n",
    "    return embedding\n",
    "\n",
    "# Assuming you have loaded your dataframes df_req and df_resume\n",
    "job_description = df_req[\"title\"].iloc[0] + \" \" + df_req[\"description\"].iloc[0] + \" \" + ' '.join(df_req[\"must_have\"].iloc[0]*2) + \" \" + ' '.join(df_req[\"should_have\"].iloc[0]*2) + \" \" + ' '.join(df_req[\"nice_to_have\"].iloc[0])\n",
    "job_description_embedding_bert = bert_text_to_embedding(bert_tokenizer, bert_model, job_description, max_length=32)\n",
    "\n",
    "similarities = []\n",
    "\n",
    "for _, resume_row in df_resume.iterrows():\n",
    "    relevant_resume_parts = extract_relevant_sections(bert_tokenizer, bert_model, resume_row[' resume_text'], job_description_embedding_bert)\n",
    "    similarity = detailed_comparison(sbert_model, relevant_resume_parts, job_description)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "df_resume['similarity'] = similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41902d04-b8a4-4dc7-9ed9-26fcb92a0148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2daf95-ec84-4f49-9477-2e022459f3cc",
   "metadata": {},
   "source": [
    "# Strategy III\n",
    "\n",
    "Combining all these techniques, like BERT, combined with Named entity recognition, part-of-speech tagging (POS), and sentiment analysis, can be used to extract relevant information. And later on, with multiple testing, we can design a scoring system that gives different importances to a different part of the job. Right now, most things are treated equally unless specified otherwise. The system could be more robust by deeply analyzing the text and identifying parts of speech that do not contribute to calculating matching scores.\n",
    "\n",
    "Here's the pre-processing function with NER, but because I don't have any validation data, so I don't know whether I should use this or not. Because it is slow and might decrease performance. In the above code replace the below-commented function, if you wish to run NER.\n",
    "\n",
    "**NOTE:** This needs spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0c498-0542-44bc-9b74-0b29ea36261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def process_string(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a given string by translating to English, removing punctuations,\n",
    "    removing specific words ('a'), tokenizing, lemmatizing each word,\n",
    "    removing stopwords, and removing named entities.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The processed string.\n",
    "    \"\"\"\n",
    "    # Translating the text to English\n",
    "    translator = Translator(to_lang='en')\n",
    "    text = translator.translate(text)\n",
    "\n",
    "    # Converting text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing punctuations from the text\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "\n",
    "    # Removing the word 'a' if it appears as a separate word (using regex '\\ba+\\b')\n",
    "    text = re.sub(r'\\ba+\\b', '', text)\n",
    "\n",
    "    # Tokenizing the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatizing and removing named entities\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        doc = nlp(token)\n",
    "        if not doc.ents:\n",
    "            processed_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    # Joining the processed tokens back into a single string\n",
    "    processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366ac0e-50b8-47c4-b0f5-112769c4ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0625cb2-2d90-45f4-986f-499ab0d7aa9f",
   "metadata": {},
   "source": [
    "# Strategy IV\n",
    "\n",
    "Use LLM, I tested LLMs, and they can easily get all the relevant information form all the continuous resume text. Once we have the text, we can use simple keyword and rule-based matching to generate a score for each resume or even use the entire above pipeline, this will wokr the best because now every irrelevant information would have already been removed. To pull this of we can combine all the above strategy and can create a hybrid model out of it.\n",
    "\n",
    "I didn't use this approach as this requires access to CHATGPT API, which is a paid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2f0b2-ae52-4b01-a647-b9ab072da907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
